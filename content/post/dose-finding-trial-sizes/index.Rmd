---
title: "Sample sizes in phase I"
subtitle: "Empirically, what sample size is used in dose-escalation trials?"
summary: "Empirically, what sample size is used in dose-escalation trials?"
authors: 
  - admin
categories:
- Research
tags:
  - DoseFinding
projects: 
  - DoseFindingData
date: '2020-05-07'
lastmod: '2020-05-07T15:57:56Z'
draft: false
featured: no
image:
  caption: 'Photo by [Joshua Coleman on Unsplash](https://unsplash.com/photos/_yVRLC75Ma8)'
  focal_point: ''
  preview_only: no
bibliography: library.bib
---

# Introduction

I recently had reason to estimate the average size of a dose-escalation trial. 
Based on my own experience, my immediate answer was "about 30 patients".
However, using the data of @BrockDoseFindingData introduced in [this recent post](post/dose-finding-data/), there is no reason to guess. 
The dataset contains dose-level outcomes from 122 phase I clinical trial manuscripts reporting results of 139 dose-escalation experiments in cancer between 2008 and 2014.

Perhaps surprisingly, we did not record the sample size of each trial.
The focus of the research was not phase I trials per se, but the outcomes seen at individual doses in phase I trials.
For that reason, we recorded the number of patients evaluable at each dose _for several outcomes_, including toxicity and efficacy outcomes.
As [this post explains](post/dose-finding-data/), the outcome most commonly reported was incidence of dose-limiting toxicity (DLT), with about 95% of studies reporting dose-level DLT data.
Thus, we can derive the number of patients in a dose-escalation study by summing the numbers of patients at each dose.


# Empirical sample size of phase I trials
With those caveats out the way, let's load the data:
```{r, cache=TRUE, message=FALSE}
source('https://raw.githubusercontent.com/brockk/dosefindingdata/master/Load.R')
```

and some required packages:

```{r, message=FALSE}
library(dplyr)
library(ggplot2)
```

and then calculate the number of patients in each study:

```{r}
dlt_evaluable <- binary_events %>% 
  filter(OutcomeId == 1) %>% # This is DLT
  group_by(Study) %>% 
  summarise(NumPatients = sum(n), NumDoses = n()) 
```

We can simply visualise those summarised data:
```{r, fig.width=7.5, fig.height=5}
dlt_evaluable %>% 
  ggplot(aes(x = NumPatients)) + 
  stat_count(aes(y = ..prop..), alpha = 0.5) + 
  geom_density(col = 'blue', size = 1.3) + 
  xlim(0, NA) + 
  labs(x = 'Total sample size', y = 'Proportion')
```

to learn that the modal size is about $n=20$.
The distribution has a pronounced positive skew with a small number of relatively large sample sizes seen. 

Calculating summary statistics:
```{r}
dlt_evaluable %>% 
  pull(NumPatients) %>% 
  summary()
```

we see that the median size is 23 patients with an inter-quartile range of (18, 33).
My initial guess of about 30 was a bit toppy.

## By drug type
The database also records various descriptive variables about the clinical scenarios.
We can, for example, investigate sample size by the type of drug that is having its dose varied.

Let's do that.
First, let us check which treatment types are contained:

```{r}
dlt_evaluable %>% 
  left_join(studies, by = 'Study') %>%
  count(DoseVaryingTreatmentType) %>% 
  arrange(-n) %>% 
  head(5) %>% knitr::kable(digits = 1)
```

We see that this period yielded trials mostly of chemotherapy and inhibitor drugs.
It really only makes sense to summarise sample sizes for those two categories:

```{r, fig.width=7.5, fig.height=10}
dlt_evaluable %>% 
  left_join(studies, by = 'Study') %>%
  filter(DoseVaryingTreatmentType %in% c('Chemotherapy', 'Inhibitor')) %>% 
  ggplot(aes(x = NumPatients)) + 
  stat_count(aes(y = ..prop..), alpha = 0.5) + 
  geom_density(col = 'blue', size = 1.3) + 
  xlim(0, NA) + 
  facet_wrap(~ DoseVaryingTreatmentType, ncol = 1) + 
  labs(x = 'Total sample size', y = 'Proportion')
```

The distributions look fairly exchangeable, suggesting that phase I trials of inhibitors have tended to use similar sizes to those of chemotherapies.

## Haematological vs non-haematological

We can instead contrast the sample sizes of trials in haematological and solid tumour (or non-haematological) diseases:

```{r, fig.width=7.5, fig.height=10}
dlt_evaluable %>% 
  left_join(studies, by = 'Study') %>%
  filter(HaemNonhaem %in% c('Haematological', 'NonHaematological')) %>% 
  ggplot(aes(x = NumPatients)) + 
  stat_count(aes(y = ..prop..), alpha = 0.5) + 
  geom_density(col = 'blue', size = 1.3) + 
  xlim(0, NA) + 
  facet_wrap(~ HaemNonhaem, ncol = 1) + 
  labs(x = 'Total sample size', y = 'Proportion')
```

Again, we see that the distributions are largely coincident.

## Sample-size per dose investigated

We can plot sample size against the number of dose investigated to learn roughly how many patients are evaluated at each dose:

```{r, fig.width=7.5, fig.height=5}
dlt_evaluable %>% 
  ggplot(aes(x = NumDoses, y = NumPatients)) + 
  geom_point() + 
  geom_smooth(method = 'loess') + 
  xlim(0, NA)
```

In one particular study, each patient received a different dose.
That is the outlier on the right of the plot above.
If we exclude that point, we get a better look at the relationship:

```{r, fig.width=7.5, fig.height=5}
dlt_evaluable %>% 
  filter(NumDoses < 20) %>% 
  ggplot(aes(x = NumDoses, y = NumPatients)) + 
  geom_point() + 
  geom_smooth(method = 'loess') + 
  xlim(0, NA)
```

For numbers of doses less than about 10, the aggregate sample size is broadly linear in the number of doses.
How many patients per dose?

```{r}
dlt_evaluable %>% 
  filter(NumDoses < 20) %>% 
  lm(NumPatients ~ NumDoses, data = .) %>% 
  broom::tidy() %>% knitr::kable()
```

About 4, with (3, 5) being a good working uncertainty interval.
That is not to say that phase I trials should be that size, of course!
Merely a reflection of what was done, on average, between 2008 and 2014.


## References

